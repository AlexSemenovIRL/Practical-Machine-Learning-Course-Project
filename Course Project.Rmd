Practical Machine Learning Course Project
========================================================
## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, I am going to use the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participant. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The five ways are exactly according to the specification [A], throwing the elbows to the front [B], lifting the dumbbell only halfway [C], lowering the dumbbell only halfway [D] and throwing the hips to the front [E]. Please note that Class A corresponds to correct performance of the escercise. The goal of this project is to predict the manner in which the participants carry out the exercise. You can find more infromation about the study [here](http://groupware.les.inf.puc-rio.br/har)

## Getting and Cleaning Data

The training data for this project can be found here [download](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
The test data are can be found here [download](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)
The data for this project can be found at the [source](http://groupware.les.inf.puc-rio.br/har)

### Load Libraries
```{r}
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(gbm))
suppressMessages(library(survival))
suppressMessages(library(splines))
suppressMessages(library(parallel))
suppressMessages(library(plyr))
```

### Get the data
```{r}
set.seed(666)

# You can download the data directly using the code below
#TrainSet <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), na.strings=c("NA",""), header=T)
#TestSet <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), na.strings=c("NA",""), header=T)

# load data from local machine
TrainSet <- read.csv("pml-training.csv", na.strings=c("NA",""), header=T)
TestSet <- read.csv("pml-testing.csv", na.strings=c("NA",""), header=T)
```

### Compare the Train and Test Sets for Column Names
```{r}
all(names(TrainSet[1:length(TrainSet)-1]) == names(TestSet[1:length(TestSet)-1]))
```

### Identify near zero variance predictors

The *nearZeroVar* diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: 
  1. they have very few unique values relative to the number of samples 
  2. the ratio of the frequency of the most common value to the frequency of the second most common value is large. 
  3. *checkConditionalX* looks at the distribution of the columns of x conditioned on the levels of y and identifies columns of x that are sparse within groups of y.

```{r}
nzv <- nearZeroVar(TrainSet, saveMetrics = TRUE)
TrainSet <- TrainSet[,nzv$nzv == FALSE]
```

### Split the Initial Training set into Training and Validation Sets with ratio 70/30
```{r}
Partition <- createDataPartition(TrainSet$classe, p = 0.7, list=FALSE)
TrainingSet <- TrainSet[Partition,]
ValidationSet <- TrainSet[-Partition,]
```

### Remove variables that are most NAs, here we use 95% of them should be NA
```{r}
RemoveNA <- sapply(TrainingSet, function(x) mean(is.na(x))) > 0.95
TrainingSet <- TrainingSet[,RemoveNA==F]
ValidationSet <- ValidationSet[,RemoveNA==F]
```

### Finally,take out the variables that don't make too much sence to use
```{r}
TrainingSet <- TrainingSet[, -(1:7)]
ValidationSet <- ValidationSet[, -(1:7)]
```

## Model Selection and Validation

### Model #1 Random Forests
```{r}
set.seed(666)
RF_Model <- randomForest(classe ~ ., data = TrainingSet)
RF_Prediction <- predict(RF_Model, ValidationSet, type = "class")
RF_ConfusionMatrix <- confusionMatrix(RF_Prediction, ValidationSet$classe)
RF_ConfusionMatrix
```

### Plot the error rate as a function of # of trees for RF Model
```{r}
plot(RF_Model);box();grid()
```
### Display variable importance
```{r}
RF_Importance <- data.frame(Variable = names(RF_Model$importance[,1]), Importance = RF_Model$importance[,1])
RF_Importance <- RF_Importance[order(-RF_Importance[,2]),]
RF_Importance
```

### Model #2 GBM Model
```{r}
set.seed(666)
Control <- trainControl(method = "cv",5)
GBM_Model <- train(classe ~ ., data = TrainingSet, method = "gbm",trControl = Control,verbose = FALSE)

GBM_Prediction <- predict(GBM_Model, newdata = ValidationSet)
GBM_ConfusionMatrix <- confusionMatrix(GBM_Prediction, ValidationSet$classe)
GBM_ConfusionMatrix
```
### Plot the error rate as a function of number of trees for GBM Model
```{r}
plot(GBM_Model)
```
### Compare the two models
```{r}
RF_Summary <- round(RF_ConfusionMatrix$overall,4)
GBM_Summary <- round(GBM_ConfusionMatrix$overall,4)

ModelComparison <- rbind(RF_Summary,GBM_Summary)
ModelComparison[,1:4]
```
1. As can be seen the Random Forest Models is a better option with accuaracy of **99.41%**
2. Our out-of-sample error rate is 0.59%, as we used almost all of the variables
3. We can probably get a better estimate if we would have ran PCA or if we selected only most influential variables in RF_Importance

## Predicting Results on the Test Data

### Evaluating two models
```{r}
Result <- predict(RF_Model, TestSet[, -length(names(TestSet))])
Result
```

## Conclusion

Based on the analysis we found that there are many variables with empty values in the data set. Through analysis we also found that random forest model have better accuracy than GBM model in the above analysis.
